---
layout: post
title: Generative Adverserial Networks (GANs)
date: 2024-11-20 19:04:00+0900
description: GANs
tags: formatting links
disqus_comments: true
categories: Deep Learning
---

GAN은 데이터의 분포를 adverserial 방식으로 implicit하게 학습하는 generative model입니다.

## Likelihood-free training
Autoregressive model, VAE, Normalizing flow, Diffusion은 Maximum Likelihood Estimator (MLE)를 사용하여 parameters를 optimize합니다. 데이터를 충분히 가지고 있다면 MLE는 Minimum Variance Unbiased Estimaor (MVUE)를 만족합니다. 충분한 샘플 데이터를 가지면 MLE는 이론적으로 estimator입니다. 하지만 likelihood가 높다고 능사는 아닙니다. 

>"For imperfect models, achieving high log-likelihoods might not always imply good sample quality, and vice versa. (Theis et al., 2016)

Two-sample Test는 두 독립적인 집단에 대해 통계적 특성이 서로 동일한지 여부를 검정하는 통계적 방법입니다. Test statistic이 threshold를 초과하면 null hypothesis를 reject하고 alternative hypothesis를 accept합니다. 눈 여겨 볼 부분은, 이 방법이 likelihood를 사용하지 않는다는 점입니다. Sample과 test statistic만 사용합니다. 이처럼 GAN은 likelihood-free traning이기 때문에 $$P_{data}$$를 implicit하게 학습합니다.

### Discriminator
Likelihood를 사용하지 않는 학습을 위해 test statistic을 minimize하는 objective를 설정할 수 있습니다. 하지만 test statistic을 최소화하기 쉽지 않습니다. 동일한 분포를 따르더라도 평균이 다를 수 있으며, 평균이 같아도 분산이 다를 수 있고, 평균과 분산이 같아도 분포가 다를 수 있기 때문입니다. 그래서 두 샘플이 각각 어느 분포에서 왔는지 자동으로 학습시킵니다. 이 부분이 GAN에서 discriminator라고 불리는 classifier입니다.

Discriminator $$\mathcal{D_{\phi}}$$는 아래의 objective를 따릅니다. 

$$
\max_{\mathcal{D}_{\phi}}V(P_{\theta}, \mathcal{D}_{\phi}) = \mathbb{E}_{x \sim P_{data}}[\text{log}\mathcal{D}_{\phi}(x)] + \mathbb{E}_{x \sim P_{\theta}}[\text{log}(1 - \mathcal{D}_{\phi}(x))]
$$

주어진 샘플이 real(1)인지 fake(0)인지 구분하는게 $$\mathcal{D_{\phi}}$$의 objective입니다. $$\mathcal{D_{\phi}}$$는 베르누이 분포의 파라미터를 estimate합니다.

Optimal $$\mathcal{D_{\phi}}$$는 다음과 같습니다.

$$
\mathcal{D^{*}_{\phi}} = \frac{P_{data}}{P_{data} + P_{\theta}}.
$$

$$
\text{If } P_{data} = P_{\theta} \text{, then } \mathcal{D^{*}_{\phi}} = \frac{1}{2}.
$$

$$\mathcal{D^{*}_{\phi}}$$는 $$\mathcal{D_{\phi}}$$의 objective function을 variation하면 쉽게 유도됩니다.

### Generator

Test statistic을 minimize하는 objective를 가진 generative model을 만드는게 목표입니다. Test statistic을 minimize하기 어려워서 $$\mathcal{D_{\phi}}$$를 이용했습니다. 이제 $$\mathcal{D_{\phi}}$$를 이용해서 test statistic을 minimize하는 generative model을 만듭니다.

$$
\min_{\mathcal{G_{\theta}}}\max_{\mathcal{D}_{\phi}}V(\mathcal{G_{\theta}}, \mathcal{D}_{\phi}) = \mathbb{E}_{x \sim P_{data}}[\text{log}\mathcal{D}_{\phi}(x)] + \mathbb{E}_{x \sim \mathcal{G_{\theta}}}[\text{log}(1 - \mathcal{D}_{\phi}(x))]
$$

$$\mathcal{D_{\phi}}$$의 objective에 $$P_{\theta}$$ 대신 Generator $$\mathcal{G_{\theta}}$$를 사용합니다. 위 식은 GAN에서 사용하는 loss function이 됩니다. GAN은 $$\mathcal{G_{\theta}}$$와 $$\mathcal{D_{\phi}}$$가 minimax로 경쟁하며 학습합니다. 그래서 adverserial network라고 부릅니다.

## Jensen–Shannon divergence (JSD)


$$
D_{JSD}\left[p, q\right] = \frac{1}{2}\left(D_{KL}\left[p, \frac{p + q}{2}\right] + D_{KL}\left[q, \frac{p + q}{2}\right]\right)
$$

1. $$D_{JSD}\left[p, q\right] \ge 0$$
2. $$D_{JSD}\left[p, q\right] = 0 \text{ iif. } p=q$$
3. $$D_{JSD}\left[p, q\right] = D_{JSD}\left[q, p\right] $$

GAN은 $$P_{data}$$와 $$P_{\theta}$$의 확률 분포를 JSD를 사용해 근사시킵니다. 
만약 $$\mathcal{D^{*}_{\phi}}$$를 가지고 있다면,

$$
V(\mathcal{G}_{\theta}, \mathcal{D}^{*}_{\mathcal{G}_{\theta}}(x)) = 2D_{JSD}\left[P_{data}, P_{\theta}\right] - \log{4}.
$$


$$\mathcal{G}^{*}_{\theta}$$도 가지고 있다면,

$$
V(\mathcal{G}^{*}_{\theta}, \mathcal{D}^{*}_{\mathcal{G}_{\theta}}(x)) = -\log{4}.
$$

이 됩니다.

## Training GAN

GAN을 학습하는 방식은 다음과 같다.
1. $$\mathcal{D}$$에서 $$x$$를 샘플링한다.
2. $$\mathcal{G}$$에서 $$z$$를 샘플링한다.
3. $$\phi$$를 gradient ascent로 업데이트한다.
4. $$\theta$$를 gradient descent로 업데이트한다.
5. 반복한다.

## Pros, Cons, and Limitations

### Pros.
1. No likelihood. Don't need to know the density.
   - Density를 명시적으로 알 필요가 없습니다.
2. Flexibility for neural network architecture. We haven't constraint the architecture of G.
   - $$\mathcal{G}$$의 신경망 구조에 대한 제약이 없습니다.
3. Fast sampling. Sing forward pass through G.
   - $$\mathcal{G}$$만으로 샘플을 빠르게 생성할 수 있습니다.

### Cons.
1. Very difficult to train in practice.

### Unstable optimization
{% include figure.liquid path="assets/img/2024-11-20-GAN/loss.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
$$\mathcal{D}$$와 $$\mathcal{G}$$의 경쟁적인 구조 때문에 최적화 과정이 매우 불안정합니다. Oscillating이 매우 심합니다.

### Mode collapse
$$\mathcal{G}$$가 일부 샘플만 생성하여 다양성이 부족한 샘플을 생성하는 현상이 발생할 수 있습니다.


### Evaluation
GAN의 성능을 평가할 객관적인 지표를 정의하기 어렵습니다.

---

## f-divergence.
Divergence의 일반화된 형태인 f-divergence는 다음과 같이 정의됩니다.

$$
D_{f}(p, q) = \mathbb{E}_{x \sim q}\left[f\left(\frac{p(x)}{q(x)}\right)\right] \text{, where }
$$

- $$f$$ is convex.
- $$f$$ is lower-semicontinuous.
- $$f(1) = 0$$.

{% include figure.liquid path="assets/img/2024-11-20-GAN/f-divergence.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}

$$
\min_{\mathcal{G}_{\theta}}\max_{\mathcal{D}_{\phi}} F(\theta, \phi) = \mathbb{E}_{x \sim P_{data}}\left[\mathcal{T}_{\phi}(x)\right] - \mathbb{E}_{x \sim P_{\theta}}\left[f^{*}(\mathcal{T}_{\phi}(x))\right] \text{ (Nowozin et al., 2016.)}
$$



학습 초반에 \mathcal{G}가 생성하는 샘플이랑 훈련 데이터가 차이가 나서 $$p$$와 $$q$$가 support를 공유하지 않아 discontinuity 문제가 발생한다. 그래서 "Smoother" distance가 필요하고, 이 역할을 하는 것이 Wasserstein Distance입니다.

## Wasserstein distance

$$
D_{w}(p, q) = \inf\limits_{\gamma \in \prod} \mathbb{E}_{(x, y) \sim \gamma}\left[ \left\| x - y \right\| \right]
$$

- $$\prod$$은 $$x$$, $$y$$에 대한 가능한 모든 joint distributions 집합입니다.
- $$x$$와 $$y$$의 marginal 분포는 각각 $$p(x)$$, $$q(y)$$를 만족해야 합니다.
  - $$\text{marginal of } x \text{ is } p(x) = \int \gamma(x, y)dy$$.
  - $$\text{marginal of } y \text{ is } p(y) = \int \gamma(x, y)dx$$.
- $$\gamma(y \mid x) \text{ : a probabilistic earth moving plan that warps } p(x) \text{ to } q(y)$$.

## Kantorovich-Rubinstein duality.

$$
D_{w}(p, q) = \sup\limits_{\left\|f\right\|_{L} \le 1} \mathbb{E}_{x \sim p}\left[ f(x) \right] - \mathbb{E}_{x \sim q} \left[ f(x) \right]
$$

$$\left\|f\right\|_{L} \le 1$$ means the Lipschitz constant of $$f(x)$$ is 1.

technically, $$\forall x, y: \mid f(x) - f(y) \mid \le \left\| x - y \right\|_{1}$$.
Intuitively, $$f$$ cannot change too rapidly.

scalar function f를 optimization하는 problem is equal to the wasserstein distance. (right-hand side를 최적화 -> wasserstein distance랑 같은 결과.)


Wasserstein GAN with $$\mathcal{D}_{\phi}(x)$$ and $$\mathcal{G}_{\theta}(z)$$

$$
\min_{\theta} \max_{\phi} \mathbb{E}_{x \sim P_{data}} \left[ \mathcal{D}_{\phi}(x) \right] - \mathbb{E}_{q \sim P(z)} \left[ \mathcal{D}_{\phi} \left( \mathcal{G}_{\theta}(z) \right) \right]
$$

Lipschitzness of $$\mathcal{D}_{\phi}(x)$$ is enforced through weight clipping or gradient penalty on $$\mathcal{D}_{x}(x)$$


Vanila GAN
```python
# Generator
class Generator(nn.Module):
    def __init__(self, latent_dim, output_size):
        super(Generator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.ReLU(inplace=True),
            nn.Linear(256, 512),
            nn.ReLU(inplace=True),
            nn.Linear(512, 1024),
            nn.ReLU(inplace=True),
            nn.Linear(1024, output_size),
            nn.Tanh()  # Output values in range [-1, 1]
        )

    def forward(self, z):
        return self.model(z)

# Discriminator
class Discriminator(nn.Module):
    def __init__(self, input_size):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_size, 1024),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(1024, 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(256, 1),
            nn.Sigmoid()  # Output single probability
        )

    def forward(self, x):
        return self.model(x)
```

DCGAN
```python
# Generator
class Generator(nn.Module):
    def __init__(self, latent_dim):
        super(Generator, self).__init__()
        self.init_size = 8  # Starting image size: 8x8
        self.fc = nn.Linear(latent_dim, 128 * self.init_size * self.init_size)

        self.model = nn.Sequential(
            nn.BatchNorm2d(128),
            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1),  # Upsample to 16x16
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # Upsample to 32x32
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),  # Output RGB image
            nn.Tanh()  # [-1, 1] output range
        )

    def forward(self, z):
        out = self.fc(z)
        out = out.view(out.size(0), 128, self.init_size, self.init_size)  # Reshape to (batch, 128, 8, 8)
        return self.model(out)

# Discriminator
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),  # Downsample to 16x16
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # Downsample to 8x8
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # Downsample to 4x4
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Flatten(),
            nn.Linear(256 * 4 * 4, 1),  # Fully connected layer
            nn.Sigmoid()  # Output probability
        )

    def forward(self, img):
        return self.model(img)
```
{% include figure.liquid path="assets/img/2024-11-20-GAN/dcgan_result.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
