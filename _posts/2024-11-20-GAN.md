---
layout: post
title: Generative Adverserial Networks (GANs)
date: 2024-11-20 19:04:00+0900
description: GANs
tags: formatting links
disqus_comments: true
categories: Deep Learning
---

GAN은 데이터의 분포를 adverserial 방식으로 implicit하게 학습하는 generative model입니다.

## Generative model
Generative model에 대한 설명은 여기를 참고.

## Likelihood-free training
Autoregressive model, VAE, Normalizing flow, Diffusion은 Maximum Likelihood Estimator (MLE)를 사용하여 parameters를 optimize합니다. 데이터를 충분히 가지고 있다면 MLE는 Minimum Variance Unbiased Estimaor (MVUE)를 만족합니다. 충분한 샘플 데이터를 가지면 MLE는 이론적으로 estimator입니다. 하지만 likelihood가 높다고 능사는 아닙니다. 

>"For imperfect models, achieving high log-likelihoods might not always imply good sample quality, and vice versa. (Theis et al., 2016)

Two-sample Test는 두 독립적인 집단에 대해 통계적 특성이 서로 동일한지 여부를 검정하는 통계적 방법입니다. Test statistic이 threshold를 초과하면 null hypothesis를 reject하고 alternative hypothesis를 accept합니다. 눈 여겨 볼 부분은, 이 방법이 likelihood를 사용하지 않는다는 점입니다. Sample과 test statistic만 사용합니다. 이처럼 GAN은 likelihood-free traning이기 때문에 $$P_{data}$$를 implicit하게 학습합니다.

### Discriminator
Likelihood를 사용하지 않는 학습을 위해 test statistic을 minimize하는 objective를 설정할 수 있습니다. 하지만 test statistic을 최소화하기 쉽지 않습니다. 동일한 분포를 따르더라도 평균이 다를 수 있으며, 평균이 같아도 분산이 다를 수 있고, 평균과 분산이 같아도 분포가 다를 수 있기 때문입니다. 그래서 두 샘플이 각각 어느 분포에서 왔는지 자동으로 학습시킵니다. 이 부분이 GAN에서 discriminator라고 불리는 classifier입니다.

Discriminator $$\mathcal{D_{\phi}}$$는 아래의 objective를 따릅니다. 

$$
\max_{\mathcal{D}_{\phi}}V(P_{\theta}, \mathcal{D}_{\phi}) = \mathbb{E}_{x \sim P_{data}}[\text{log}\mathcal{D}_{\phi}(x)] + \mathbb{E}_{x \sim P_{\theta}}[\text{log}(1 - \mathcal{D}_{\phi}(x))]
$$

주어진 샘플이 real(1)인지 fake(0)인지 구분하는게 $$\mathcal{D_{\phi}}$$의 objective입니다. $$\mathcal{D_{\phi}}$$는 베르누이 분포의 파라미터를 estimate합니다.

Optimal $$\mathcal{D_{\phi}}$$는 다음과 같습니다.

$$
\mathcal{D^{*}_{\phi}} = \frac{P_{data}}{P_{data} + P_{\theta}}.
$$

$$
\text{If } P_{data} = P_{\theta} \text{, then } \mathcal{D^{*}_{\phi}} = \frac{1}{2}.
$$

$$\mathcal{D^{*}_{\phi}}$$는 $$\mathcal{D_{\phi}}$$의 objective function을 variation하면 쉽게 유도됩니다.

### Generator

Test statistic을 minimize하는 objective를 가진 generative model을 만드는게 목표입니다. Test statistic을 minimize하기 어려워서 $$\mathcal{D_{\phi}}$$를 이용했습니다. 이제 $$\mathcal{D_{\phi}}$$를 이용해서 test statistic을 minimize하는 generative model을 만듭니다.

$$
\min_{\mathcal{G_{\theta}}}\max_{\mathcal{D}_{\phi}}V(\mathcal{G_{\theta}}, \mathcal{D}_{\phi}) = \mathbb{E}_{x \sim P_{data}}[\text{log}\mathcal{D}_{\phi}(x)] + \mathbb{E}_{x \sim \mathcal{G_{\theta}}}[\text{log}(1 - \mathcal{D}_{\phi}(x))]
$$

$$\mathcal{D_{\phi}}$$의 objective에 $$P_{\theta}$$ 대신 Generator $$\mathcal{G_{\theta}}$$를 사용합니다. 위 식은 GAN에서 사용하는 loss function이 됩니다. GAN은 $$\mathcal{G_{\theta}}$$와 $$\mathcal{D_{\phi}}$$가 minimax로 경쟁하며 학습합니다. 그래서 adverserial network라고 부릅니다.

## Jensen–Shannon divergence (JSD)


$$
D_{JSD}\left[p, q\right] = \frac{1}{2}\left(D_{KL}\left[p, \frac{p + q}{2}\right] + D_{KL}\left[q, \frac{p + q}{2}\right]\right)
$$

1. $$D_{JSD}\left[p, q\right] \ge 0$$
2. $$D_{JSD}\left[p, q\right] = 0 \text{ iif. } p=q$$
3. $$D_{JSD}\left[p, q\right] = D_{JSD}\left[q, p\right] $$

GAN은 $$P_{data}$$와 $$P_{\theta}$$의 확률 분포를 JSD를 사용해 근사시킵니다. 
만약 $$\mathcal{D^{*}_{\phi}}$$를 가지고 있다면,

$$
V(\mathcal{G}_{\theta}, \mathcal{D}^{*}_{\mathcal{G}_{\theta}}(x)) = 2D_{JSD}\left[P_{data}, P_{\theta}\right] - \log{4}.
$$


$$\mathcal{G}^{*}_{\theta}$$도 가지고 있다면,

$$
V(\mathcal{G}^{*}_{\theta}, \mathcal{D}^{*}_{\mathcal{G}_{\theta}}(x)) = -\log{4}.
$$

## Training GAN

GAN을 학습하는 방식은 다음과 같다.
1. $$\mathcal{D}$$에서 $$x$$를 샘플링한다.
2. $$\mathcal{G}$$에서 $$z$$를 샘플링한다.
3. $$\phi$$를 gradient ascent로 업데이트한다.
4. $$\theta$$를 gradient descent로 업데이트한다.
5. 반복한다.

Pros.
1. No likelihood. Don't need to know the density.
2. Flexibility for neural network architecture. We haven't constraint the architecture of G.
3. Fast sampling. Sing forward pass through G.

Cons.
1. Very difficult to train in practice.

GAN의 세 가지 Challenges가 있다.
1. Unstable optimization
2. Mode collapse
3. Evaluation


{% include figure.liquid path="assets/img/2024-11-20-GAN/loss.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
첫 번째 문제점은 oscillating이 심하다는거다. 위 그래프 보고 어디서 멈춰야할지 감이 안온다.

두 번째 문제점은 G collapses to one or few samples라는거다. D가 너무 학습이 잘돼서 G가 학습이 안된다.

세 번째 문제점은 


---

Likelihood-free training이기 때문에 특별한 density의 architecture를 선택할 필요가 없다.

f-divergence.
지금까지 두 가지 divergnce가 나왔다. KLD, JSD.

Divergence를 일반화한 방식이 f-divergence다. 

$$
D_{f}(p, q) = \mathbb{E}_{x \sim q}\left[f\left(\frac{p(x)}{q(x)}\right)\right] \text{, where f is convex and lower-semicontinuous with f(1) = 0.}
$$

{% include figure.liquid path="assets/img/2024-11-20-GAN/f-divergence.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}

Nowozin et al., 2016.


f-divergence를 잘 유도하면 f-GAN 식이 나온다.

$$
\min_{\mathcal{G}_{\theta}}\max_{\mathcal{D}_{\phi}} F(\theta, \phi) = \mathbb{E}_{x \sim P_{data}}\left[\mathcal{T}_{\phi}(x)\right] - \mathbb{E}_{x \sim P_{\theta}}\left[f^{*}(\mathcal{T}_{\phi}(x))\right]
$$

막간 support

$$ supp(p) = { x \in \mathbb{R}^{n} \mid p(x) > 0 }$$
확률분포 P의 support는 P(x)가 0보다 큰 x 집합을 의미한다. 유니폼 분포 U[a, b]의 support는 [a, b]다.

1. $$\frac{p(x)}{q(x)}$$에서 $$q(x) = 0$$이고 $$p(x)>0$$인 지점이 있다면 수렴하지 않고 발산하거나 정의되지 않음. Discontinuity 문제.
2. $$q(x)$$의 support가 $$p(x)$$의 suport를 cover 해야함. ($$p(x) > 0 \longrightarrow q(x)>0$$). 그렇지 않으면 발산하거나 정의되지 않음. (Discontinuity).

$$
D_{f}(p, q)= \mathcal{E}_{x \sim q} \left[f\left(\frac{p(x)}{q(x)}\right)\right]
$$

p랑 q가 suupport를 share하지 않음. 학습 초반에 G가 생성하는 샘플이랑 훈련 데이터가 많이 달라서 발생함.

needed a "smmother" distance D(p, q) that is defined when p and q have disjoint supports.
support를 cover하지 않는, 즉 discontinuity problem이 arise할 때 어떻게 해결할 것인가 -> wasserstein distance.

wasserstein distance

$$
D_{w}(p, q) = \inf\limits_{\gamma \in \prod} \mathbb{E}_{(x, y) \sim \gamma}\left[ \left\| x - y \right\| \right]
$$

$$
\text{,where } \prod \text{contains all possible joint distributions of } (x, y).
$$

$$
\text{marginal of } x \text{ is } p(x) = \int \gamma(x, y)dy.
$$

$$
\text{marginal of } y \text{ is } p(y) = \int \gamma(x, y)dx.
$$

$$
\gamma(y \mid x) \text{ : a probabilistic earth moving plan that warps } p(x) \text{ to } q(y).
$$

Kantorovich-Rubinstein duality.

$$
D_{w}(p, q) = \sup\limits_{\left\|f\right\|_{L} \le 1} \mathbb{E}_{x \sim p}\left[ f(x) \right] - \mathbb{E}_{x \sim q} \left[ f(x) \right]
$$

$$\left\|f\right\|_{L} \le 1$$ means the Lipschitz constant of $$f(x)$$ is 1.

technically, $$\forall x, y: \mid f(x) - f(y) \mid \le \left\| x - y \right\|_{1}$$.
Intuitively, $$f$$ cannot change too rapidly.

scalar function f를 optimization하는 problem is equal to the wasserstein distance. (right-hand side를 최적화 -> wasserstein distance랑 같은 결과.)


Wasserstein GAN with $$\mathcal{D}_{\phi}(x)$$ and $$\mathcal{G}_{\theta}(z)$$

$$
\min_{\theta} \max_{\phi} \mathbb{E}_{x \sim P_{data}} \left[ \mathcal{D}_{\phi}(x) \right] - \mathbb{E}_{q \sim P(z)} \left[ \mathcal{D}_{\phi} \left( \mathcal{G}_{\theta}(z) \right) \right]
$$

Lipschitzness of $$\mathcal{D}_{\phi}(x)$$ is enforced through weight clipping or gradient penalty on $$\mathcal{D}_{x}(x)$$


Vanila GAN
```python
# Generator
class Generator(nn.Module):
    def __init__(self, latent_dim, output_size):
        super(Generator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.ReLU(inplace=True),
            nn.Linear(256, 512),
            nn.ReLU(inplace=True),
            nn.Linear(512, 1024),
            nn.ReLU(inplace=True),
            nn.Linear(1024, output_size),
            nn.Tanh()  # Output values in range [-1, 1]
        )

    def forward(self, z):
        return self.model(z)

# Discriminator
class Discriminator(nn.Module):
    def __init__(self, input_size):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_size, 1024),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(1024, 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(256, 1),
            nn.Sigmoid()  # Output single probability
        )

    def forward(self, x):
        return self.model(x)
```

DCGAN
```python
# Generator
class Generator(nn.Module):
    def __init__(self, latent_dim):
        super(Generator, self).__init__()
        self.init_size = 8  # Starting image size: 8x8
        self.fc = nn.Linear(latent_dim, 128 * self.init_size * self.init_size)

        self.model = nn.Sequential(
            nn.BatchNorm2d(128),
            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1),  # Upsample to 16x16
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # Upsample to 32x32
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),  # Output RGB image
            nn.Tanh()  # [-1, 1] output range
        )

    def forward(self, z):
        out = self.fc(z)
        out = out.view(out.size(0), 128, self.init_size, self.init_size)  # Reshape to (batch, 128, 8, 8)
        return self.model(out)

# Discriminator
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),  # Downsample to 16x16
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # Downsample to 8x8
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # Downsample to 4x4
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Flatten(),
            nn.Linear(256 * 4 * 4, 1),  # Fully connected layer
            nn.Sigmoid()  # Output probability
        )

    def forward(self, img):
        return self.model(img)
```
{% include figure.liquid path="assets/img/2024-11-20-GAN/dcgan_result.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
