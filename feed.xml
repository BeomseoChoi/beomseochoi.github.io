<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://beomseochoi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://beomseochoi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-26T13:54:03+00:00</updated><id>https://beomseochoi.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Generative Adverserial Networks (GANs)</title><link href="https://beomseochoi.github.io/blog/2024/GAN/" rel="alternate" type="text/html" title="Generative Adverserial Networks (GANs)"/><published>2024-11-20T10:04:00+00:00</published><updated>2024-11-20T10:04:00+00:00</updated><id>https://beomseochoi.github.io/blog/2024/GAN</id><content type="html" xml:base="https://beomseochoi.github.io/blog/2024/GAN/"><![CDATA[<p>GAN은 데이터의 분포를 adverserial 방식으로 implicit하게 학습하는 generative model입니다.</p> <h2 id="likelihood-free-training">Likelihood-free training</h2> <p>Autoregressive model, VAE, Normalizing flow, Diffusion은 Maximum Likelihood Estimator (MLE)를 사용하여 parameters를 optimize합니다. 데이터를 충분히 가지고 있다면 MLE는 Minimum Variance Unbiased Estimaor (MVUE)를 만족합니다. 충분한 샘플 데이터를 가지면 MLE는 이론적으로 estimator입니다. 하지만 likelihood가 높다고 능사는 아닙니다.</p> <blockquote> <p>“For imperfect models, achieving high log-likelihoods might not always imply good sample quality, and vice versa. (Theis et al., 2016)</p> </blockquote> <p>Two-sample Test는 두 독립적인 집단에 대해 통계적 특성이 서로 동일한지 여부를 검정하는 통계적 방법입니다. Test statistic이 threshold를 초과하면 null hypothesis를 reject하고 alternative hypothesis를 accept합니다. 눈 여겨 볼 부분은, 이 방법이 likelihood를 사용하지 않는다는 점입니다. Sample과 test statistic만 사용합니다. 이처럼 GAN은 likelihood-free traning이기 때문에 \(P_{data}\)를 implicit하게 학습합니다.</p> <h3 id="discriminator">Discriminator</h3> <p>Likelihood를 사용하지 않는 학습을 위해 test statistic을 minimize하는 objective를 설정할 수 있습니다. 하지만 test statistic을 최소화하기 쉽지 않습니다. 동일한 분포를 따르더라도 평균이 다를 수 있으며, 평균이 같아도 분산이 다를 수 있고, 평균과 분산이 같아도 분포가 다를 수 있기 때문입니다. 그래서 두 샘플이 각각 어느 분포에서 왔는지 자동으로 학습시킵니다. 이 부분이 GAN에서 discriminator라고 불리는 classifier입니다.</p> <p>Discriminator \(\mathcal{D_{\phi}}\)는 아래의 objective를 따릅니다.</p> \[\max_{\mathcal{D}_{\phi}}V(P_{\theta}, \mathcal{D}_{\phi}) = \mathbb{E}_{x \sim P_{data}}[\text{log}\mathcal{D}_{\phi}(x)] + \mathbb{E}_{x \sim P_{\theta}}[\text{log}(1 - \mathcal{D}_{\phi}(x))]\] <p>주어진 샘플이 real(1)인지 fake(0)인지 구분하는게 \(\mathcal{D_{\phi}}\)의 objective입니다. \(\mathcal{D_{\phi}}\)는 베르누이 분포의 파라미터를 estimate합니다.</p> <p>Optimal \(\mathcal{D_{\phi}}\)는 다음과 같습니다.</p> \[\mathcal{D^{*}_{\phi}} = \frac{P_{data}}{P_{data} + P_{\theta}}.\] \[\text{If } P_{data} = P_{\theta} \text{, then } \mathcal{D^{*}_{\phi}} = \frac{1}{2}.\] <p>\(\mathcal{D^{*}_{\phi}}\)는 \(\mathcal{D_{\phi}}\)의 objective function을 variation하면 쉽게 유도됩니다.</p> <h3 id="generator">Generator</h3> <p>Test statistic을 minimize하는 objective를 가진 generative model을 만드는게 목표입니다. Test statistic을 minimize하기 어려워서 \(\mathcal{D_{\phi}}\)를 이용했습니다. 이제 \(\mathcal{D_{\phi}}\)를 이용해서 test statistic을 minimize하는 generative model을 만듭니다.</p> \[\min_{\mathcal{G_{\theta}}}\max_{\mathcal{D}_{\phi}}V(\mathcal{G_{\theta}}, \mathcal{D}_{\phi}) = \mathbb{E}_{x \sim P_{data}}[\text{log}\mathcal{D}_{\phi}(x)] + \mathbb{E}_{x \sim \mathcal{G_{\theta}}}[\text{log}(1 - \mathcal{D}_{\phi}(x))]\] <p>\(\mathcal{D_{\phi}}\)의 objective에 \(P_{\theta}\) 대신 Generator \(\mathcal{G_{\theta}}\)를 사용합니다. 위 식은 GAN에서 사용하는 loss function이 됩니다. GAN은 \(\mathcal{G_{\theta}}\)와 \(\mathcal{D_{\phi}}\)가 minimax로 경쟁하며 학습합니다. 그래서 adverserial network라고 부릅니다.</p> <h2 id="jensenshannon-divergence-jsd">Jensen–Shannon divergence (JSD)</h2> \[D_{JSD}\left[p, q\right] = \frac{1}{2}\left(D_{KL}\left[p, \frac{p + q}{2}\right] + D_{KL}\left[q, \frac{p + q}{2}\right]\right)\] <ol> <li> \[D_{JSD}\left[p, q\right] \ge 0\] </li> <li> \[D_{JSD}\left[p, q\right] = 0 \text{ iif. } p=q\] </li> <li> \[D_{JSD}\left[p, q\right] = D_{JSD}\left[q, p\right]\] </li> </ol> <p>GAN은 \(P_{data}\)와 \(P_{\theta}\)의 확률 분포를 JSD를 사용해 근사시킵니다. 만약 \(\mathcal{D^{*}_{\phi}}\)를 가지고 있다면,</p> \[V(\mathcal{G}_{\theta}, \mathcal{D}^{*}_{\mathcal{G}_{\theta}}(x)) = 2D_{JSD}\left[P_{data}, P_{\theta}\right] - \log{4}.\] <p>\(\mathcal{G}^{*}_{\theta}\)도 가지고 있다면,</p> \[V(\mathcal{G}^{*}_{\theta}, \mathcal{D}^{*}_{\mathcal{G}_{\theta}}(x)) = -\log{4}.\] <p>이 됩니다.</p> <h2 id="training-gan">Training GAN</h2> <p>GAN을 학습하는 방식은 다음과 같다.</p> <ol> <li>\(\mathcal{D}\)에서 \(x\)를 샘플링한다.</li> <li>\(\mathcal{G}\)에서 \(z\)를 샘플링한다.</li> <li>\(\phi\)를 gradient ascent로 업데이트한다.</li> <li>\(\theta\)를 gradient descent로 업데이트한다.</li> <li>반복한다.</li> </ol> <h2 id="pros-cons-and-limitations">Pros, Cons, and Limitations</h2> <h3 id="pros">Pros.</h3> <ol> <li>No likelihood. Don’t need to know the density. <ul> <li>Density를 명시적으로 알 필요가 없습니다.</li> </ul> </li> <li>Flexibility for neural network architecture. We haven’t constraint the architecture of G. <ul> <li>\(\mathcal{G}\)의 신경망 구조에 대한 제약이 없습니다.</li> </ul> </li> <li>Fast sampling. Sing forward pass through G. <ul> <li>\(\mathcal{G}\)만으로 샘플을 빠르게 생성할 수 있습니다.</li> </ul> </li> </ol> <h3 id="cons">Cons.</h3> <ol> <li>Very difficult to train in practice.</li> </ol> <h3 id="unstable-optimization">Unstable optimization</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-20-GAN/loss-480.webp 480w,/assets/img/2024-11-20-GAN/loss-800.webp 800w,/assets/img/2024-11-20-GAN/loss-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-11-20-GAN/loss.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>\(\mathcal{D}\)와 \(\mathcal{G}\)의 경쟁적인 구조 때문에 최적화 과정이 매우 불안정합니다. Oscillating이 매우 심합니다.</p> <h3 id="mode-collapse">Mode collapse</h3> <p>\(\mathcal{G}\)가 일부 샘플만 생성하여 다양성이 부족한 샘플을 생성하는 현상이 발생할 수 있습니다.</p> <h3 id="evaluation">Evaluation</h3> <p>GAN의 성능을 평가할 객관적인 지표를 정의하기 어렵습니다.</p> <hr/> <h2 id="f-divergence">f-divergence.</h2> <p>Divergence의 일반화된 형태인 f-divergence는 다음과 같이 정의됩니다.</p> \[D_{f}(p, q) = \mathbb{E}_{x \sim q}\left[f\left(\frac{p(x)}{q(x)}\right)\right] \text{, where }\] <ul> <li>\(f\) is convex.</li> <li>\(f\) is lower-semicontinuous.</li> <li>\(f(1) = 0\).</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-20-GAN/f-divergence-480.webp 480w,/assets/img/2024-11-20-GAN/f-divergence-800.webp 800w,/assets/img/2024-11-20-GAN/f-divergence-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-11-20-GAN/f-divergence.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> \[\min_{\mathcal{G}_{\theta}}\max_{\mathcal{D}_{\phi}} F(\theta, \phi) = \mathbb{E}_{x \sim P_{data}}\left[\mathcal{T}_{\phi}(x)\right] - \mathbb{E}_{x \sim P_{\theta}}\left[f^{*}(\mathcal{T}_{\phi}(x))\right] \text{ (Nowozin et al., 2016.)}\] <p>학습 초반에 \mathcal{G}가 생성하는 샘플이랑 훈련 데이터가 차이가 나서 \(p\)와 \(q\)가 support를 공유하지 않아 discontinuity 문제가 발생한다. 그래서 “Smoother” distance가 필요하고, 이 역할을 하는 것이 Wasserstein Distance입니다.</p> <h2 id="wasserstein-distance">Wasserstein distance</h2> \[D_{w}(p, q) = \inf\limits_{\gamma \in \prod} \mathbb{E}_{(x, y) \sim \gamma}\left[ \left\| x - y \right\| \right]\] <ul> <li>\(\prod\)은 \(x\), \(y\)에 대한 가능한 모든 joint distributions 집합입니다.</li> <li>\(x\)와 \(y\)의 marginal 분포는 각각 \(p(x)\), \(q(y)\)를 만족해야 합니다. <ul> <li>\(\text{marginal of } x \text{ is } p(x) = \int \gamma(x, y)dy\).</li> <li>\(\text{marginal of } y \text{ is } p(y) = \int \gamma(x, y)dx\).</li> </ul> </li> <li>\(\gamma(y \mid x) \text{ : a probabilistic earth moving plan that warps } p(x) \text{ to } q(y)\).</li> </ul> <h2 id="kantorovich-rubinstein-duality">Kantorovich-Rubinstein duality.</h2> \[D_{w}(p, q) = \sup\limits_{\left\|f\right\|_{L} \le 1} \mathbb{E}_{x \sim p}\left[ f(x) \right] - \mathbb{E}_{x \sim q} \left[ f(x) \right]\] <p>\(\left\|f\right\|_{L} \le 1\) means the Lipschitz constant of \(f(x)\) is 1.</p> <p>technically, \(\forall x, y: \mid f(x) - f(y) \mid \le \left\| x - y \right\|_{1}\). Intuitively, \(f\) cannot change too rapidly.</p> <p>scalar function f를 optimization하는 problem is equal to the wasserstein distance. (right-hand side를 최적화 -&gt; wasserstein distance랑 같은 결과.)</p> <p>Wasserstein GAN with \(\mathcal{D}_{\phi}(x)\) and \(\mathcal{G}_{\theta}(z)\)</p> \[\min_{\theta} \max_{\phi} \mathbb{E}_{x \sim P_{data}} \left[ \mathcal{D}_{\phi}(x) \right] - \mathbb{E}_{q \sim P(z)} \left[ \mathcal{D}_{\phi} \left( \mathcal{G}_{\theta}(z) \right) \right]\] <p>Lipschitzness of \(\mathcal{D}_{\phi}(x)\) is enforced through weight clipping or gradient penalty on \(\mathcal{D}_{x}(x)\)</p> <p>Vanila GAN</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Generator
</span><span class="k">class</span> <span class="nc">Generator</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Generator</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">output_size</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Tanh</span><span class="p">()</span>  <span class="c1"># Output values in range [-1, 1]
</span>        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="c1"># Discriminator
</span><span class="k">class</span> <span class="nc">Discriminator</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Discriminator</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Sigmoid</span><span class="p">()</span>  <span class="c1"># Output single probability
</span>        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <p>DCGAN</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Generator
</span><span class="k">class</span> <span class="nc">Generator</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Generator</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">init_size</span> <span class="o">=</span> <span class="mi">8</span>  <span class="c1"># Starting image size: 8x8
</span>        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="mi">128</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">init_size</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">init_size</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ConvTranspose2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># Upsample to 16x16
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ConvTranspose2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># Upsample to 32x32
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># Output RGB image
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">Tanh</span><span class="p">()</span>  <span class="c1"># [-1, 1] output range
</span>        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">128</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">init_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">init_size</span><span class="p">)</span>  <span class="c1"># Reshape to (batch, 128, 8, 8)
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

<span class="c1"># Discriminator
</span><span class="k">class</span> <span class="nc">Discriminator</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Discriminator</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># Downsample to 16x16
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># Downsample to 8x8
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># Downsample to 4x4
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">256</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>  <span class="c1"># Fully connected layer
</span>            <span class="n">nn</span><span class="p">.</span><span class="nc">Sigmoid</span><span class="p">()</span>  <span class="c1"># Output probability
</span>        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">img</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-20-GAN/dcgan_result-480.webp 480w,/assets/img/2024-11-20-GAN/dcgan_result-800.webp 800w,/assets/img/2024-11-20-GAN/dcgan_result-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-11-20-GAN/dcgan_result.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>]]></content><author><name></name></author><category term="Deep"/><category term="Learning"/><category term="formatting"/><category term="links"/><summary type="html"><![CDATA[GANs]]></summary></entry><entry><title type="html">Support</title><link href="https://beomseochoi.github.io/blog/2024/Support/" rel="alternate" type="text/html" title="Support"/><published>2024-11-20T10:04:00+00:00</published><updated>2024-11-20T10:04:00+00:00</updated><id>https://beomseochoi.github.io/blog/2024/Support</id><content type="html" xml:base="https://beomseochoi.github.io/blog/2024/Support/"><![CDATA[<p>막간 support</p> <p>\(supp(p) = { x \in \mathbb{R}^{n} \mid p(x) &gt; 0 }\) 확률분포 P의 support는 P(x)가 0보다 큰 x 집합을 의미한다. 유니폼 분포 U[a, b]의 support는 [a, b]다.</p> <ol> <li>\(\frac{p(x)}{q(x)}\)에서 \(q(x) = 0\)이고 \(p(x)&gt;0\)인 지점이 있다면 수렴하지 않고 발산하거나 정의되지 않음. Discontinuity 문제.</li> <li>\(q(x)\)의 support가 \(p(x)\)의 suport를 cover 해야함. (\(p(x) &gt; 0 \longrightarrow q(x)&gt;0\)). 그렇지 않으면 발산하거나 정의되지 않음. (Discontinuity).</li> </ol> \[D_{f}(p, q)= \mathcal{E}_{x \sim q} \left[f\left(\frac{p(x)}{q(x)}\right)\right]\] <p>p랑 q가 suupport를 share하지 않음. 학습 초반에 G가 생성하는 샘플이랑 훈련 데이터가 많이 달라서 발생함.</p> <hr/>]]></content><author><name></name></author><category term="Statistics"/><category term="formatting"/><category term="links"/><summary type="html"><![CDATA[GANs]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://beomseochoi.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://beomseochoi.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://beomseochoi.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://beomseochoi.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://beomseochoi.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://beomseochoi.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>